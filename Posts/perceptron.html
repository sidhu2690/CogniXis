<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="sidharth">
    <meta name="date" content="2025-05-06">
    <title>Perceptron Intuition: From Biology to Machine Learning</title>

    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-XJH8CP1D4R"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-XJH8CP1D4R');
</script>

    <!-- Include Polyfill for broader browser support -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <!-- Include MathJax for equations -->
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>

    <script>
        // Highlight active section in sidebar on scroll
        document.addEventListener("scroll", function () {
            const sections = document.querySelectorAll("section");
            const links = document.querySelectorAll(".sidebar-link");
            let index = sections.length;
            while(--index && window.scrollY + 60 < sections[index].offsetTop) {}
            links.forEach((link) => link.classList.remove("active"));
            links[index].classList.add("active");
        });
    </script>

    <style>
        body {
            font-family: 'Google Sans', Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f9f9f9;
            line-height: 1.6;
        }
        .sidebar {
            width: 220px;
            position: fixed;
            top: 0;
            left: 0;
            height: 100%;
            background: #fff;
            border-right: 1px solid #ddd;
            overflow-y: auto;
            padding-top: 1rem;
        }
        .sidebar h2 {
            margin-left: 20px;
            font-size: 18px;
        }
        .sidebar ul {
            list-style: none;
            padding: 0;
        }
        .sidebar-link {
            display: block;
            padding: 10px 20px;
            color: #333;
            text-decoration: none;
            transition: background-color 0.3s;
        }
        .sidebar-link:hover {
            background-color: #f1f1f1;
        }
        .sidebar-link.active {
            background-color: #e0f0ff;
            font-weight: bold;
            color: #2588df;
        }
        .content {
            margin-left: 240px;
            padding: 2rem;
            max-width: 800px;
        }
        section { margin-bottom: 2rem; }
        h1, h2, h3, h4 { color: #333; }
        pre { background: #f4f4f4; padding: 1rem; overflow-x: auto; }
        code { font-family: Consolas, monospace; }
        .center-figure {
            display: flex;
            flex-direction: column;
            align-items: center;
            margin: 1.5rem 0;
        }
        .center-figure img {
            max-width: 100%;
            height: auto;
        }
        figcaption {
            font-size: 0.8em;
            margin-top: 4px;
            text-align: center;
            color: #555;
            font-style: italic;
        }
        .styled-table {
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 0.9em;
            width: 100%;
        }
        .styled-table th,
        .styled-table td {
            border: 1px solid #ddd;
            padding: 8px;
        }
        .styled-table th {
            background-color: #2588df;
            color: #fff;
            text-align: left;
        }
        .styled-table tbody tr:hover { background-color: #f1f1f1; }
        .styled-table tbody tr.active-row { font-weight: bold; color: #2588df; }

        @media screen and (max-width: 768px) {
            .sidebar {
                position: static;
                width: 100%;
                height: auto;
                border-right: none;
                border-bottom: 1px solid #ddd;
                display: flex;
                flex-direction: row;
                flex-wrap: wrap;
                justify-content: space-around;
            }
            .sidebar ul {
                display: flex;
                flex-wrap: wrap;
                justify-content: center;
                padding: 0;
                margin: 0;
            }
            .sidebar-link {
                padding: 10px;
                font-size: 0.9em;
            }
            .content {
                margin-left: 0;
                padding: 1rem;
            }
        }

        @media screen and (max-width: 480px) {
            body {
                font-size: 14px;
            }
            .sidebar-link {
                padding: 8px;
            }
            .center-figure {
                margin: 1rem 0;
            }
            pre {
                font-size: 0.85em;
                padding: 0.75rem;
            }
        }
    </style>
</head>
<body id="top">

<aside class="sidebar">
  <h2><a href="https://sidhu2690.github.io/CogniXis/" style="color: inherit; text-decoration: none;">CogniXis</a></h2>
  <ul>
    <li><a href="#introduction" class="sidebar-link">1. Introduction</a></li>
    <li><a href="#biological-inspiration" class="sidebar-link">2. Biological Inspiration</a></li>
    <li><a href="#physical-intuition" class="sidebar-link">3. Physical Intuition</a></li>
    <li><a href="#perceptron-basics" class="sidebar-link">4. Perceptron Basics</a></li>
    <li><a href="#perceptron-advanced" class="sidebar-link">5. Perceptron – Going Beyond Straight Lines</a></li>
    <li><a href="#activation-functions" class="sidebar-link">6. Activation Functions</a></li>
    <li><a href="#multilayer-perceptron" class="sidebar-link">7. Multilayer Perceptron and Neural Networks
</a></li>
    <li><a href="#code-example" class="sidebar-link">8. Code Example</a></li>
    <li><a href="#conclusion" class="sidebar-link">9. Conclusion</a></li>
  </ul>
</aside>

    <div class="content">
        <header>
            <h1>Perceptron Intuition: From Biology to Machine Learning</h1>
            <p><em>Exploring the perceptron’s roots in biology, its mechanics, and its evolution into modern machine learning concepts.</em></p>
        </header>

        <section id="introduction">
            <h2>1. Introduction</h2>
            <p>Hey there! Today, we’re diving into something super cool: the perceptron. It’s one of the simplest yet most foundational ideas in machine learning, and it’s got a story that starts with the human brain and ends with computers making decisions. In this post, we’ll walk through its biological inspiration, how it works with weights and biases, and even code one from scratch using NumPy. Plus, we’ll connect it to linear models like house price prediction and explore why activation functions matter. Ready? Let’s get started!</p>
        </section>


<section id="biological-inspiration">
    <h2>2. Biological Inspiration</h2>
    <p>Let’s kick things off with the biological neuron—the real MVP behind the perceptron. Think of it like this: in your brain, neurons are tiny cells that help you think, move, and feel. They pass messages around using electrical signals, kind of like how messages travel through wires.</p>
    <p>Each neuron has parts that do specific jobs. <strong>Dendrites</strong> are like little branches that catch signals coming in from other neurons. These signals go to the <strong>cell body</strong>, which adds them up. If the signals are strong enough, the neuron decides to fire—meaning it sends its own signal out through a long part called the <strong>axon</strong>.</p>
    <p>At the end of the axon, the message jumps across a tiny gap called a <strong>synapse</strong> to the next neuron. If the message is strong, the next neuron might fire too—and the process keeps going. If not, it stays quiet.</p>
    <p>This fire-or-don’t-fire behavior is what inspired the perceptron. The perceptron is a super simple version of this idea. It takes some inputs, checks how strong they are (with weights), adds a little extra push or pull (the bias), and decides whether to "fire" or not. If it does, it gives an output of 1; if not, it gives 0. Simple, right? That’s the idea we’ll build on!</p>
    <div class="center-figure">
        <img src="assets\perceptron\neuron.png" alt="Biological Neuron">
        <figcaption>Figure 1: Diagram of a biological neuron with dendrites, cell body, and axon.</figcaption>
    </div>
</section>


<section id="physical-intuition">
  <h2>3. Physical Intuition: Predicting House Prices Like a Human Would</h4>

  <p>
    Before we dive into perceptrons and activation functions, let’s first build an intuition for what “learning from data” really means—without diving straight into code or formulas.
  </p>

  <p>
    Imagine you’ve just moved to a new city and want to buy a house. You’re clueless about the pricing trends. Naturally, you visit a local broker who gives you a dataset: recently sold houses with details like:
  </p>

  <ul>
    <li><strong>Area</strong> (in square meters)</li>
    <li><strong>Number of rooms</strong></li>
    <li><strong>Number of bathrooms</strong></li>
    <li><strong>Location coordinates</strong></li>
    <li><strong>Price</strong> (in thousands of dollars)</li>
  </ul>

  <p>
    You decide to start simple. You pick just the <strong>area</strong> feature and plot it against the price.
    To your surprise, the dots on the graph form something close to a straight line! This suggests a linear relationship: the larger the house, the more it costs.
  </p>

  <div class="center-figure">
    <img src="assets/perceptron/house.png" alt="House Price vs Area">
    <figcaption>Figure 2: Plot of house prices versus area, showing a near-linear relationship.</figcaption>
  </div>

  <p>
    Encouraged, you fit a straight line through these points—using Excel’s trendline tool or a Python function like <code>scipy.stats.linregress</code>. The line follows this general form:
  </p>

  <p style="text-align: center; font-size: 1.2em;">
    $$ \text{price} = w_1 \cdot \text{area} + w_0 $$
  </p>

  <p>
    This is a simple linear equation:
  </p>

  <ul>
    <li><strong>\( w_1 \)</strong>: the slope, which tells you how much the price changes for every additional square meter.</li>
    <li><strong>\( w_0 \)</strong>: the intercept, representing the base price when area is zero.</li>
  </ul>

<p>
    Once you’ve got these values, you can estimate the price of any house just by plugging in its area. For instance, if:
</p>

\[
\begin{aligned}
&\text{Given:} \quad w_1 = 0.2, \quad w_0 = 50, \quad \text{area} = 1000 \\
&\text{price} = 50 + 0.2 \times 1000 = 250
\end{aligned}
\]
<p>
    That’s great! But houses aren't only defined by area. What if we add more features like the number of rooms, bathrooms, and location?
    Now the equation becomes:
</p>

<p style="text-align: center; font-size: 1.2em;">
    $$ y = w_1 x_1 + w_2 x_2 + w_3 x_3 + \dots + w_n x_n + w_0 $$
</p>

<p>
    Each \( x_i \) is a feature (like area, rooms, location), and each \( w_i \) is a weight showing how much that feature affects the price.
    The \( w_0 \) is still the intercept or bias term, a base value added to every prediction.
</p>
  <p>
    But here’s a challenge: it’s easy to plot price vs. area on a 2D graph. It’s also doable to plot price vs. two features using a 3D plot. But what if we use 4, 5, or 10 features?
    We can’t visualize that anymore. It’s beyond human perception.
  </p>

  <p>
    And this brings us to the real problem in machine learning: <strong>how do we find the correct weights \( w_1, w_2, \dots, w_n \) when we can’t even see the data properly?</strong>
  </p>

  <p>
    As the number of features grows, the space of possible solutions becomes vast and complex. It’s like trying to balance on a tightrope in ten dimensions.
    That’s why we need algorithms—not just to fit a line, but to find the best line (or surface, or curve) in a space we can’t visualize.
  </p>

  <p>
    This is exactly what models like the <strong>perceptron</strong> are designed to do. They take in multiple features, assign weights, and learn over time how to tweak those weights to minimize error.
    But now that we understand what weights and bias really mean, we’re ready to talk about the perceptron itself.
  </p>
</section>




<section id="perceptron-basics">
    <h2>4. Perceptron Basics</h2>

    <p>Now that we’ve seen how we can predict house prices using a simple formula, let’s turn that same idea into a general mathematical model. This is actually how the early version of the perceptron works. Instead of using just one feature like area, we can now include multiple features—like area, number of rooms, and location—and give each one a weight based on how important it is.</p>

    <p>So if we have features \( x_1, x_2, \ldots, x_n \), we assign weights \( w_1, w_2, \ldots, w_n \), and add a bias term \( b \). The output is calculated like this:</p>

    <p style="text-align: center;">
        \( y = w_1x_1 + w_2x_2 + \cdots + w_nx_n + b \)
    </p>

    <p>Here, \( y \) could be anything—like the price of a house. This is how the perceptron starts: just adding up weighted inputs and a bias. It’s a basic way to model relationships in data.</p>

    <div class="center-figure">
        <img src="assets/perceptron/perceptron.png" alt="Linear Model Perceptron Illustration">
        <figcaption>Figure 3: A basic mathematical model with weights and bias, like the early perceptron.</figcaption>
    </div>

    <h3>But what if we don’t want a number—we want a decision?</h3>

    <p>Great question! Sometimes, instead of predicting a value like price, we want the model to make a decision—for example, should we go for a picnic today or not?</p>

    <p>This kind of task is called <strong>classification</strong>. It means we want to sort inputs into different categories—in this case, “yes” or “no.”</p>

    <p>Just like before, we use the formula with weights and bias, but instead of using the raw number directly, we check whether it’s above or below a certain threshold.</p>

    <p>To do that, we use something called a <strong>step function</strong> or <strong>activation function</strong>. For simple binary classification, a step function is often used. It works like this:</p>

    <p style="text-align: center;">
        \[
        \text{output} =
        \begin{cases}
        1 & \text{if } y \geq 0 \\
        0 & \text{if } y < 0
        \end{cases}
        \]
    </p>

    <p>This means: if the result is positive, we output a 1 (“yes”); otherwise, we output a 0 (“no”). This is how we turn a raw number into a decision.</p>

    <h3>Let’s Try an Example</h3>

    <p>We want to classify if it’s a good day for a picnic, based on two things:</p>
    <ul>
        <li>Temperature (in °C)</li>
        <li>Chance of rain (in %)</li>
    </ul>

    <p>The perceptron has learned from previous data that:</p>
    <ul>
        <li>Weight for temperature: \( w_1 = 0.4 \)</li>
        <li>Weight for rain: \( w_2 = -0.6 \)</li>
        <li>Bias: \( b = -5 \)</li>
    </ul>

    <p>Now suppose today’s weather is:</p>
    <ul>
        <li>Temperature: \( x_1 = 25^\circ C \)</li>
        <li>Rain chance: \( x_2 = 10\% \)</li>
    </ul>

    <p>Let’s plug everything into the formula:</p>

    <p style="text-align: center;">
        \[
        y = (0.4 \times 25) + (-0.6 \times 10) + (-5)
        \]
        \[
        y = 10 - 6 - 5 = -1
        \]
    </p>

    <p>Now we apply the step function:</p>

    <p style="text-align: center;">
        \[
        y = -1 \Rightarrow \text{output} = 0
        \]
    </p>

    <p>Since the result is negative, the perceptron outputs <strong>0</strong>—which means: <strong>“Not a good day for a picnic.”</strong></p>

    <p>Let’s try a sunnier scenario. Say the temperature is 30°C and the rain chance is 5%:</p>

    <p style="text-align: center;">
        \[
        y = (0.4 \times 30) + (-0.6 \times 5) + (-5)
        \]
        \[
        y = 12 - 3 - 5 = 4
        \]
    </p>

    <p>Now, \( y = 4 \) which is positive, so:</p>

    <p style="text-align: center;">
        \[
        \text{output} = 1
        \]
    </p>

    <p>This time the perceptron says: <strong>“Yes, it’s a great day for a picnic!”</strong></p>

    <div class="center-figure">
        <img src="assets/perceptron/boundary.png" alt="Perceptron Decision Boundary">
        <figcaption>Figure 4: A perceptron draws a decision boundary to separate yes/no outcomes.</figcaption>
    </div>

    <h3>Bonus: What if we want probabilities?</h3>
    <p>The step function is great for yes/no answers, but it’s a bit harsh—it jumps instantly from 0 to 1. Sometimes, we want a smoother decision, like a confidence level. For that, we use the <strong>logistic function</strong>, also called the <strong>sigmoid</strong>:</p>

    <p style="text-align: center;">
        \[
        \sigma(y) = \frac{1}{1 + e^{-y}}
        \]
    </p>

    <p>This gives us values between 0 and 1. We can then say: if the result is above 0.5, it’s class 1; otherwise, it’s class 0. But we’ll explore this more in the next section!</p>
</section>





<section id="perceptron-advanced">
    <h2>5. Perceptron – Going Beyond Straight Lines</h2>

    <p>Cool! So far, we’ve used the perceptron to solve both <strong>regression</strong> (like predicting house prices) and <strong>classification</strong> (like deciding whether to go for a picnic).</p>

    <p>But there’s one big limitation: <strong>everything is just a straight line.</strong> We’re simply summing the weighted inputs and bias. That’s great when the relationship between inputs and output is linear—but what if it’s not?</p>

    <p>Imagine plotting house price vs. area and the points don’t follow a straight line—they curve, or form a complex pattern. In that case, our current model won’t work well, because it's only drawing straight lines or planes in higher dimensions.</p>

    <div class="center-figure">
        <img src="assets/perceptron/non_linear.png" alt="Linear Model Failing on Non-Linear Data">
        <figcaption>Figure 5: A linear model trying to fit non-linear data—it doesn’t match well.</figcaption>
    </div>

    <h3>So What Do We Do?</h3>

    <p>This is where <strong>activation functions</strong> come in again. Instead of just outputting the raw sum, we pass that sum through a non-linear function—something that bends the line.</p>

    <p>This lets the model learn <strong>curved relationships</strong>, or <strong>non-linear decision boundaries</strong>. Now we’re not stuck with just straight lines—we can handle wavy or circular patterns in the data too.</p>

    <p>For example, we take the sum:</p>

    <p style="text-align: center;">
        \( z = w_1x_1 + w_2x_2 + \cdots + w_nx_n + b \)
    </p>

    <p>Then pass it through a function like the sigmoid or ReLU (we’ll learn more about these soon):</p>

    <p style="text-align: center;">
        \( a = \sigma(z) \quad \text{or} \quad a = \text{ReLU}(z) \)
    </p>

    <p>Now the output is non-linear, and that changes everything! With this setup, our model can now <strong>bend the output</strong> to better fit the real-world data.</p>

    <div class="center-figure">
        <img src="assets/perceptron/activations.png" alt="Non-Linear Activation Transforming Output">
        <figcaption>Figure 6: Passing the weighted sum through a non-linear function bends the line.</figcaption>
    </div>

    <h3>And It Gets Better: Curved Decision Boundaries!</h3>

    <p>In classification, this means we’re no longer stuck separating data with a straight line. We can now find <strong>non-linear decision boundaries</strong>—curves, blobs, spirals—whatever shape fits the data best!</p>

    <div class="center-figure">
        <img src="assets/perceptron/decision_boundary.png" alt="Curved Decision Boundary">
        <figcaption>Figure 7: A non-linear activation helps create curved boundaries to separate complex data.</figcaption>
    </div>

    <p>In short, activation functions give the perceptron <strong>flexibility</strong>—the ability to adapt to more complex patterns. Without them, we’d be stuck in the land of straight lines forever.</p>

  </section>


        <section id="activation-functions">
            <h2>6. Activation Functions</h2>
    <p>So, now that you’ve seen why activation functions matter, let’s dive into the most popular ones. Each has its own flavor, strengths, and quirks.</p>

    <h3>1. ReLU (Rectified Linear Unit)</h3>
    <p><strong>Equation:</strong> 
        <span style="font-family:serif;">\( f(x) = \max(0, x) \)</span>
    </p>
    <p>
        This one’s super popular in modern neural networks. It just outputs the input if it’s positive, or zero if it’s not. Simple and fast!
    </p>
    <p><strong>Why it’s great:</strong> It avoids complex math and speeds up training.</p>
    <p><strong>But:</strong> It can "die"—if a neuron keeps getting negative inputs, it outputs 0 forever (called the <em>dying ReLU</em> problem).</p>

    <h3>2. Sigmoid</h3>
    <p><strong>Equation:</strong> 
        <span style="font-family:serif;">\( f(x) = \frac{1}{1 + e^{-x}} \)</span>
    </p>
    <p>
        This squashes everything between 0 and 1. It’s smooth and was once the go-to activation, especially in binary classification problems.
    </p>
    <p><strong>Why it’s great:</strong> Easy to interpret as probability.</p>
    <p><strong>But:</strong> For very big or very small inputs, it flattens out, making learning slow (called the <em>vanishing gradient</em> problem).</p>

    <h3>3. Leaky ReLU</h3>
    <p><strong>Equation:</strong> 
        <span style="font-family:serif;">\( f(x) = \begin{cases} x & \text{if } x > 0 \\ 0.01x & \text{if } x \leq 0 \end{cases} \)</span>
    </p>
    <p>
        A small twist on ReLU—it lets a tiny bit of negative values through instead of just zero. This helps keep neurons alive during training.
    </p>
    <p><strong>Why it’s great:</strong> Fixes the “dying ReLU” issue and still trains fast.</p>
    <p><strong>But:</strong> The small slope (like 0.01) is fixed—it doesn’t learn or adapt.</p>

    <h3>4. Linear (No Activation)</h3>
    <p><strong>Equation:</strong> 
        <span style="font-family:serif;">\( f(x) = x \)</span>
    </p>
    <p>
        This is just a straight line—no bending at all. It's what we used in basic perceptrons before introducing activations.
    </p>
    <p><strong>Why it’s useful:</strong> Still works great for regression tasks where the output should be continuous (like predicting price or temperature).</p>
    <p><strong>But:</strong> On its own, it can’t model anything non-linear. So it’s mostly used only at the output layer, not hidden ones.</p>

    <div class="center-figure">
        <img src="assets/perceptron/activation_functions.png" alt="Various Activation Functions">
        <figcaption>Figure 8: Different activation functions showing how they bend input in different ways.</figcaption>
    </div>

    <p>Each activation function plays a unique role. ReLU and its variants dominate deep learning models, sigmoid still shines in classification, and linear activations work well when we just need the raw number. Choose wisely based on what your model needs to learn!</p>
</section>


<section id="multilayer-perceptron">
    <h2>7. Multilayer Perceptron and Neural Networks</h2>

    <p>So now we’ve figured out how to approximate functions and predict things—like house prices or whether it’s a good day for a picnic—using a perceptron. But what happens when the patterns get trickier? What if the relationships in our data aren’t just straight lines but twisty, curvy, or super complicated? A simple summing of inputs and passing them through a non-linear activation function might not cut it anymore. We need a bigger, more complex function to handle that. So, what do we do? We stack these neurons together!</p>
    <p>The idea is pretty simple: we take the output of one perceptron and use it as the input for another. Then we keep doing that, building layers of perceptrons. Each layer processes the data a bit more, letting us capture those wild, complex patterns step by step. It’s like passing a baton in a relay race—each runner (or layer) adds something new to the effort.</p>

    <p>Let’s break it down with some math. In a single perceptron, we had this:</p>
    <p style="text-align: center;">\[ y = f(w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b) \]</p>
    <p>Here, \( x_1, x_2, \dots, x_n \) are our inputs (like temperature or rain chance), \( w_1, w_2, \dots, w_n \) are the weights, \( b \) is the bias, and \( f \) is the activation function that bends the output a bit.</p>
    <p>Now, in a multilayer perceptron, we add more layers. Here we have three types of layers:</p>

    <ol>
        <li><strong>Input layer:</strong> Passes the raw inputs \(x_1, x_2, \dots, x_n\) into the network without weights or activations.</li>
        <li><strong>Hidden layer (second layer):</strong> A set of neurons that transforms the inputs into intermediate values:</li>
    </ol>
    <p style="text-align: center;">
    \(
    \begin{aligned}
    a_1 &= f\bigl(w_{11}x_1 + w_{12}x_2 + \dots + w_{1n}x_n + b_1\bigr), \\
    a_2 &= f\bigl(w_{21}x_1 + w_{22}x_2 + \dots + w_{2n}x_n + b_2\bigr), \\
    \vdots \\
    a_m &= f\bigl(w_{m1}x_1 + w_{m2}x_2 + \dots + w_{mn}x_n + b_m\bigr)
    \end{aligned}
    \)
    </p>
    <ol start="3">
        <li><strong>Output layer:</strong> Takes the hidden outputs \(a_1, a_2, \dots, a_m\) and produces the final prediction:</li>
    </ol>
    <p style="text-align: center;">
    \(
    y = g\bigl(v_1a_1 + v_2a_2 + \dots + v_ma_m + c\bigr)
    \)
    </p>

    <p>Here:</p>
    <ul>
        <li>\(w_{ij}\) are weights from input \(x_j\) to hidden neuron \(a_i\).</li>
        <li>\(b_i\) are biases for hidden neurons.</li>
        <li>\(f(\cdot)\) is the activation function in the hidden layer (e.g., ReLU, sigmoid).</li>
        <li>\(v_i\) are weights from hidden neuron \(a_i\) to the output.</li>
        <li>\(c\) is the bias for the output neuron.</li>
        <li>\(g(\cdot)\) is the output activation (identity for regression, sigmoid/softmax for classification).</li>
    </ul>

    <p>By adjusting all weights and biases via training (backpropagation), the MLP learns to approximate very complex functions.</p>


    <div class="center-figure">
        <img src="assets/perceptron/mlp.png" alt="Neural Network Layers">
        <figcaption>Figure 10: Input, hidden, and output layers in an MLP.</figcaption>
    </div>
</section>



 <section id="code-example">
    <h2>8. Code Example</h2>
    <p>Time to get hands-on! Let’s write a simple perceptron using NumPy. This one doesn’t learn yet—we’re manually setting the weights. Later, we’ll explore how to find the best weights automatically using a technique called <strong>gradient descent</strong>.</p>

    <h3>Step 1: Define a Perceptron</h3>
    <pre><code class="language-python">
import numpy as np

def perceptron(x, weights, bias):
    z = np.dot(weights, x) + bias
    return 1 if z >= 0 else 0
    </code></pre>
    <p>This is a basic perceptron: it takes an input vector <code>x</code>, multiplies it with weights, adds a bias, and then uses a simple step function (0 or 1 output).</p>

    <h3>Step 2: AND Gate with a Perceptron</h3>
    <p>Let’s use this perceptron to simulate an AND gate. Remember, an AND gate outputs 1 only when both inputs are 1.</p>

    <pre><code class="language-python">
# Inputs and labels for AND logic gate
inputs = [
    np.array([0, 0]),
    np.array([0, 1]),
    np.array([1, 0]),
    np.array([1, 1])
]
labels = [0, 0, 0, 1]

# Set weights and bias manually (these work for AND)
weights = np.array([1, 1])
bias = -1.5

# Test the perceptron
for x, label in zip(inputs, labels):
    output = perceptron(x, weights, bias)
    print(f"Input: {x}, Predicted: {output}, Actual: {label}")
    </code></pre>

    <p>And that’s it! You’ll see the perceptron correctly outputs 1 only when both inputs are 1—just like a real AND gate.</p>

    <h3>Step 3: Simple MLP with NumPy</h3>
    <p>Now let’s extend to a tiny multilayer perceptron (one hidden layer). We’ll do a forward pass with a ReLU hidden activation and sigmoid output.</p>
    <pre><code class="language-python">
# Define forward pass for a 2-2-1 MLP

def mlp_forward(x, W1, b1, W2, b2):
    # Hidden layer (ReLU)
    z1 = np.dot(W1, x) + b1
    a1 = np.maximum(0, z1)
    # Output layer (sigmoid)
    z2 = np.dot(W2, a1) + b2
    y = 1 / (1 + np.exp(-z2))
    return y

# Example parameters for 2 inputs, 2 hidden neurons, 1 output\W1 = np.array([[0.5, -0.6], [0.1, 0.8]])
b1 = np.array([-0.1, 0.2])
W2 = np.array([[1.2, -0.7]])
b2 = 0.3

# Test the MLP on the same inputs
for x in inputs:
    output = mlp_forward(x, W1, b1, W2, b2)
    print(f"Input: {x}, MLP output: {output:.4f}")
    </code></pre>

    <p>This simple MLP uses one hidden layer to learn nonlinear patterns. In a full training loop, we’d adjust <code>W1, b1, W2, b2</code> automatically via backpropagation.</p>
</section>


        <section id="conclusion">
            <h2>9. Conclusion</h2>
            <p>And there you have it—the perceptron, from brain cells to code. We’ve seen how it’s inspired by neurons, uses weights and bias for decisions, and ties into linear models like house price prediction. Plus, we’ve peeked at why activation functions matter when things get non-linear. It’s a small step, but it’s the foundation of so much more in machine learning. Hope you enjoyed this ride—keep tinkering and exploring!</p>
        </section>
    </div>
</body>
</html>
